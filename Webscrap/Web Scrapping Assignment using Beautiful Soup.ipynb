{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f9dbbf",
   "metadata": {},
   "source": [
    "# 1) Write a python program to display all the header tags from wikipedia.org and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00be3870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c46f06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Headers\n",
      "0                      Main Page\n",
      "1           Welcome to Wikipedia\n",
      "2  From today's featured article\n",
      "3               Did you know ...\n",
      "4                    In the news\n",
      "5                    On this day\n",
      "6       Today's featured picture\n",
      "7       Other areas of Wikipedia\n",
      "8    Wikipedia's sister projects\n",
      "9            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "def Webpage(url):\n",
    "    link = requests.get(url)\n",
    "\n",
    "    Soup = BeautifulSoup(link.content)\n",
    "\n",
    "    header_tags = Soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n",
    "    header_text = [tag.text.strip() for tag in header_tags]\n",
    "\n",
    "    df = pd.DataFrame({'Headers': header_text})\n",
    "    print(df)\n",
    "    \n",
    "Webpage('https://en.wikipedia.org/wiki/Main_Page')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0a2148",
   "metadata": {},
   "source": [
    "# 2) Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice)from https://presidentofindia.nic.in/former-presidents.htm and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcc02d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Name  \\\n",
      "0           Shri Ram Nath Kovind   \n",
      "1          Shri Pranab Mukherjee   \n",
      "2   Smt Pratibha Devisingh Patil   \n",
      "3         DR. A.P.J. Abdul Kalam   \n",
      "4           Shri K. R. Narayanan   \n",
      "5        Dr Shankar Dayal Sharma   \n",
      "6            Shri R Venkataraman   \n",
      "7               Giani Zail Singh   \n",
      "8      Shri Neelam Sanjiva Reddy   \n",
      "9       Dr. Fakhruddin Ali Ahmed   \n",
      "10  Shri Varahagiri Venkata Giri   \n",
      "11              Dr. Zakir Husain   \n",
      "12  Dr. Sarvepalli Radhakrishnan   \n",
      "13           Dr. Rajendra Prasad   \n",
      "\n",
      "                                       Term of Office  \n",
      "0                      25 July, 2017 to 25 July, 2022  \n",
      "1                      25 July, 2012 to 25 July, 2017  \n",
      "2                      25 July, 2007 to 25 July, 2012  \n",
      "3                      25 July, 2002 to 25 July, 2007  \n",
      "4                      25 July, 1997 to 25 July, 2002  \n",
      "5                      25 July, 1992 to 25 July, 1997  \n",
      "6                      25 July, 1987 to 25 July, 1992  \n",
      "7                      25 July, 1982 to 25 July, 1987  \n",
      "8                      25 July, 1977 to 25 July, 1982  \n",
      "9                24 August, 1974 to 11 February, 1977  \n",
      "10  3 May, 1969 to 20 July, 1969 and 24 August, 19...  \n",
      "11                        13 May, 1967 to 3 May, 1969  \n",
      "12                       13 May, 1962 to 13 May, 1967  \n",
      "13                   26 January, 1950 to 13 May, 1962  \n"
     ]
    }
   ],
   "source": [
    "def Webpage(url):\n",
    "    link = requests.get(url)\n",
    "\n",
    "    \n",
    "    Soup = BeautifulSoup(link.content)\n",
    "\n",
    "    header_tags = Soup.find_all(['h3','p'])\n",
    "\n",
    "    header_text = [tag.text.strip() for tag in header_tags]\n",
    "    \n",
    "    president_names = []\n",
    "    terms_of_office = []\n",
    "\n",
    "    for row in header_text:\n",
    "        if '(' in row:\n",
    "            president_names.append(row.split('(')[0][0:-1])\n",
    "        if 'Term of Office' in row:\n",
    "            terms_of_office.append(row.split('Term of Office:')[1][1:])\n",
    "\n",
    "    df = pd.DataFrame({'Name': president_names, 'Term of Office': terms_of_office})\n",
    "    print(df)\n",
    "    \n",
    "Webpage('https://presidentofindia.nic.in/former-presidents.htm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4071df6e",
   "metadata": {},
   "source": [
    "# 3) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame.\n",
    "\n",
    "# a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b5cd057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Team Matches Points Rating\n",
      "0     Australia      23  2,714    118\n",
      "1      Pakistan      20  2,316    116\n",
      "2         India      33  3,807    115\n",
      "3   New Zealand      27  2,806    104\n",
      "4       England      24  2,426    101\n",
      "5  South Africa      19  1,910    101\n",
      "6    Bangladesh      25  2,451     98\n",
      "7     Sri Lanka      28  2,378     85\n",
      "8   Afghanistan      13  1,067     82\n",
      "9   West Indies      32  2,201     69\n"
     ]
    }
   ],
   "source": [
    "def Webpage(url):\n",
    "    link = requests.get(url)\n",
    "    \n",
    "    Soup = BeautifulSoup(link.content)\n",
    "    \n",
    "    Team = []\n",
    "    for i in Soup.find_all('span', class_=\"u-hide-phablet\"):\n",
    "        Team.append(i.text.strip())\n",
    "    \n",
    "    Matches = []\n",
    "    Matches_Processed = []\n",
    "    for i in Soup.find_all('td', class_=\"rankings-block__banner--matches\"):\n",
    "        Matches.append(i.text)\n",
    "    for i in Soup.find_all('td', class_=\"table-body__cell u-center-text\"):\n",
    "        Matches.append(i.text)\n",
    "    Matches_Processed = Matches[0:2]                       \n",
    "    for i in range(3,len(Matches),2):\n",
    "        Matches_Processed.append(Matches[i])     \n",
    "   \n",
    "    Points = []\n",
    "    Points_Processed = []\n",
    "    for i in Soup.find_all('td', class_=\"rankings-block__banner--points\"):\n",
    "        Points.append(i.text.strip())\n",
    "    for i in Soup.find_all('td', class_=\"table-body__cell u-center-text\"):\n",
    "        Points.append(i.text.strip())  \n",
    "    for i in range(0,len(Points),2):\n",
    "        Points_Processed.append(Points[i])\n",
    "        \n",
    "    Rating = []\n",
    "    for i in Soup.find_all('td', class_=\"rankings-block__banner--rating u-text-right\"):\n",
    "        Rating.append(i.text.strip())\n",
    "    for i in Soup.find_all('td', class_=\"table-body__cell u-text-right rating\"):\n",
    "        Rating.append(i.text.strip())\n",
    "        \n",
    "    df = pd.DataFrame({'Team':Team[0:10], 'Matches':Matches_Processed[0:10], 'Points':Points_Processed[0:10], 'Rating':Rating[0:10]})\n",
    "    print(df)\n",
    "    \n",
    "Webpage('https://www.icc-cricket.com/rankings/mens/team-rankings/odi')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4742e094",
   "metadata": {},
   "source": [
    "# 3) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame.\n",
    "\n",
    "# b) Top 10 ODI Batsmen along with the records of their team and rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0cd0ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Players Team Rating\n",
      "0                 Babar Azam  PAK    886\n",
      "1  \\nRassie van der Dussen\\n   SA    777\n",
      "2           \\nFakhar Zaman\\n  PAK    755\n",
      "3            \\nImam-ul-Haq\\n  PAK    745\n",
      "4           \\nShubman Gill\\n  IND    738\n",
      "5           \\nHarry Tector\\n  IRE    726\n",
      "6           \\nDavid Warner\\n  AUS    726\n",
      "7            \\nVirat Kohli\\n  IND    719\n",
      "8        \\nQuinton de Kock\\n   SA    718\n",
      "9           \\nRohit Sharma\\n  IND    707\n"
     ]
    }
   ],
   "source": [
    "def Webpage(url):\n",
    "    link = requests.get(url)\n",
    "    \n",
    "    Soup = BeautifulSoup(link.content)\n",
    "    \n",
    "    Players = []\n",
    "    for i in Soup.find_all('div', class_=\"rankings-block__banner--name-large\"):\n",
    "        Players.append(i.text.strip())\n",
    "    for i in Soup.find_all('td', class_=\"table-body__cell rankings-table__name name\"):\n",
    "        Players.append(i.text)\n",
    "          \n",
    "    Team = []\n",
    "    for i in Soup.find_all('div', class_=\"rankings-block__banner--nationality\"):\n",
    "        Team.append(i.text.strip())\n",
    "    for i in Soup.find_all('span', class_=\"table-body__logo-text\"):\n",
    "        Team.append(i.text)\n",
    "    \n",
    "    Rating = []\n",
    "    for i in Soup.find_all('div', class_=\"rankings-block__banner--rating\"):\n",
    "        Rating.append(i.text.strip())\n",
    "    for i in Soup.find_all('td', class_=\"table-body__cell rating\"):\n",
    "        Rating.append(i.text)\n",
    "        \n",
    "    df = pd.DataFrame({'Players':Players[0:10], 'Team':Team[0:10], 'Rating':Rating[0:10]})\n",
    "    print(df) \n",
    "    \n",
    "Webpage('https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be61dd54",
   "metadata": {},
   "source": [
    "# 3) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\n",
    "\n",
    "# c) Top 10 ODI bowlers along with the records of their team andrating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7f68fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Players Team Rating\n",
      "0    Josh Hazlewood  AUS    705\n",
      "1    Mohammed Siraj  IND    691\n",
      "2    Mitchell Starc  AUS    686\n",
      "3        Matt Henry   NZ    667\n",
      "4       Trent Boult   NZ    660\n",
      "5        Adam Zampa  AUS    652\n",
      "6       Rashid Khan  AFG    640\n",
      "7    Shaheen Afridi  PAK    630\n",
      "8  Mujeeb Ur Rahman  AFG    630\n",
      "9     Mohammad Nabi  AFG    626\n"
     ]
    }
   ],
   "source": [
    "def Webpage(url):\n",
    "    link = requests.get(url)\n",
    "    \n",
    "    Soup = BeautifulSoup(link.content)\n",
    "    \n",
    "    Players = []\n",
    "    for i in Soup.find_all('div', class_=\"rankings-block__banner--name-large\"):\n",
    "        Players.append(i.text.strip())\n",
    "    for i in Soup.find_all('td', class_=\"table-body__cell rankings-table__name name\"):\n",
    "        Players.append(i.text.strip())\n",
    "          \n",
    "    Team = []\n",
    "    for i in Soup.find_all('div', class_=\"rankings-block__banner--nationality\"):\n",
    "        Team.append(i.text.strip())\n",
    "    for i in Soup.find_all('span', class_=\"table-body__logo-text\"):\n",
    "        Team.append(i.text)\n",
    "    \n",
    "    Rating = []\n",
    "    for i in Soup.find_all('div', class_=\"rankings-block__banner--rating\"):\n",
    "        Rating.append(i.text.strip())\n",
    "    for i in Soup.find_all('td', class_=\"table-body__cell rating\"):\n",
    "        Rating.append(i.text)\n",
    "        \n",
    "    df = pd.DataFrame({'Players':Players[0:10], 'Team':Team[0:10], 'Rating':Rating[0:10]})\n",
    "    print(df) \n",
    "    \n",
    "Webpage('https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3d0ac2",
   "metadata": {},
   "source": [
    "# 4) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame.\n",
    "\n",
    "# a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ccc3ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Team Matches Points Rating\n",
      "0     Australia      21  3,603    172\n",
      "1       England      28  3,342    119\n",
      "2  South Africa      26  3,098    119\n",
      "3         India      27  2,820    104\n",
      "4   New Zealand      28  2,688     96\n",
      "5   West Indies      29  2,743     95\n",
      "6    Bangladesh      14    977     70\n",
      "7     Sri Lanka      12    820     68\n",
      "8      Thailand      12    806     67\n",
      "9      Pakistan      27  1,678     62\n"
     ]
    }
   ],
   "source": [
    "def Webpage(url):\n",
    "    link = requests.get(url)\n",
    "    \n",
    "    Soup = BeautifulSoup(link.content)\n",
    "    \n",
    "    Team = []\n",
    "    for i in Soup.find_all('span', class_=\"u-hide-phablet\"):\n",
    "        Team.append(i.text.strip())\n",
    "    \n",
    "    Matches = []\n",
    "    Matches_Processed = []\n",
    "    for i in Soup.find_all('td', class_=\"rankings-block__banner--matches\"):\n",
    "        Matches.append(i.text)\n",
    "    for i in Soup.find_all('td', class_=\"table-body__cell u-center-text\"):\n",
    "        Matches.append(i.text)\n",
    "    Matches_Processed = Matches[0:2]                       \n",
    "    for i in range(3,len(Matches),2):\n",
    "        Matches_Processed.append(Matches[i])     \n",
    "   \n",
    "    Points = []\n",
    "    Points_Processed = []\n",
    "    for i in Soup.find_all('td', class_=\"rankings-block__banner--points\"):\n",
    "        Points.append(i.text.strip())\n",
    "    for i in Soup.find_all('td', class_=\"table-body__cell u-center-text\"):\n",
    "        Points.append(i.text.strip())  \n",
    "    for i in range(0,len(Points),2):\n",
    "        Points_Processed.append(Points[i])\n",
    "        \n",
    "    Rating = []\n",
    "    for i in Soup.find_all('td', class_=\"rankings-block__banner--rating u-text-right\"):\n",
    "        Rating.append(i.text.strip())\n",
    "    for i in Soup.find_all('td', class_=\"table-body__cell u-text-right rating\"):\n",
    "        Rating.append(i.text.strip())\n",
    "        \n",
    "    df = pd.DataFrame({'Team':Team[0:10], 'Matches':Matches_Processed[0:10], 'Points':Points_Processed[0:10], 'Rating':Rating[0:10]})\n",
    "    print(df)\n",
    "    \n",
    "Webpage('https://www.icc-cricket.com/rankings/womens/team-rankings/odi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab73db93",
   "metadata": {},
   "source": [
    "# 4) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame.\n",
    "\n",
    "# b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83d13717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Players Team Rating\n",
      "0  Chamari Athapaththu   SL    758\n",
      "1          Beth Mooney  AUS    754\n",
      "2      Laura Wolvaardt   SA    732\n",
      "3       Natalie Sciver  ENG    731\n",
      "4          Meg Lanning  AUS    717\n",
      "5     Harmanpreet Kaur  IND    716\n",
      "6      Smriti Mandhana  IND    714\n",
      "7         Ellyse Perry  AUS    626\n",
      "8      Stafanie Taylor   WI    618\n",
      "9       Tammy Beaumont  ENG    595\n"
     ]
    }
   ],
   "source": [
    "def Webpage(url):\n",
    "    link = requests.get(url)\n",
    "    \n",
    "    Soup = BeautifulSoup(link.content)\n",
    "    \n",
    "    Players = []\n",
    "    for i in Soup.find_all('div', class_=\"rankings-block__banner--name-large\"):\n",
    "        Players.append(i.text.strip())\n",
    "    for i in Soup.find_all('td', class_=\"table-body__cell rankings-table__name name\"):\n",
    "        Players.append(i.text.strip())\n",
    "          \n",
    "    Team = []\n",
    "    for i in Soup.find_all('div', class_=\"rankings-block__banner--nationality\"):\n",
    "        Team.append(i.text.strip())\n",
    "    for i in Soup.find_all('span', class_=\"table-body__logo-text\"):\n",
    "        Team.append(i.text)\n",
    "    \n",
    "    Rating = []\n",
    "    for i in Soup.find_all('div', class_=\"rankings-block__banner--rating\"):\n",
    "        Rating.append(i.text.strip())\n",
    "    for i in Soup.find_all('td', class_=\"table-body__cell rating\"):\n",
    "        Rating.append(i.text)\n",
    "        \n",
    "    df = pd.DataFrame({'Players':Players[0:10], 'Team':Team[0:10], 'Rating':Rating[0:10]})\n",
    "    print(df) \n",
    "    \n",
    "Webpage('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f650f38",
   "metadata": {},
   "source": [
    "# 4) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame.\n",
    "\n",
    "# c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08abd93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Players Team Rating\n",
      "0   Hayley Matthews   WI    382\n",
      "1    Natalie Sciver  ENG    371\n",
      "2      Ellyse Perry  AUS    366\n",
      "3    Marizanne Kapp   SA    349\n",
      "4       Amelia Kerr   NZ    328\n",
      "5     Deepti Sharma  IND    322\n",
      "6  Ashleigh Gardner  AUS    292\n",
      "7     Jess Jonassen  AUS    250\n",
      "8     Sophie Devine   NZ    233\n",
      "9          Nida Dar  PAK    232\n"
     ]
    }
   ],
   "source": [
    "def Webpage(url):\n",
    "    link = requests.get(url)\n",
    "    \n",
    "    Soup = BeautifulSoup(link.content)\n",
    "    \n",
    "    Players = []\n",
    "    for i in Soup.find_all('div', class_=\"rankings-block__banner--name-large\"):\n",
    "        Players.append(i.text.strip())\n",
    "    for i in Soup.find_all('td', class_=\"table-body__cell rankings-table__name name\"):\n",
    "        Players.append(i.text.strip())\n",
    "          \n",
    "    Team = []\n",
    "    for i in Soup.find_all('div', class_=\"rankings-block__banner--nationality\"):\n",
    "        Team.append(i.text.strip())\n",
    "    for i in Soup.find_all('span', class_=\"table-body__logo-text\"):\n",
    "        Team.append(i.text)\n",
    "    \n",
    "    Rating = []\n",
    "    for i in Soup.find_all('div', class_=\"rankings-block__banner--rating\"):\n",
    "        Rating.append(i.text.strip())\n",
    "    for i in Soup.find_all('td', class_=\"table-body__cell rating\"):\n",
    "        Rating.append(i.text)\n",
    "        \n",
    "    df = pd.DataFrame({'Players':Players[0:10], 'Team':Team[0:10], 'Rating':Rating[0:10]})\n",
    "    print(df) \n",
    "    \n",
    "Webpage('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894ebbd7",
   "metadata": {},
   "source": [
    "# 5) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world andmake data frame.i) Headlineii) Timeiii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fdf1914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Headlines          Time  \\\n",
      "0   Chris Christie doubles down on Trump attacks a...   2 Hours Ago   \n",
      "1   Cohabitating after separating is not uncommon:...   3 Hours Ago   \n",
      "2   GOP presidential candidate Burgum says he does...   3 Hours Ago   \n",
      "3   Yellen had a 'constructive visit' to China wit...   4 Hours Ago   \n",
      "4   Jim Cramer calls for 'buy and homework' over '...   5 Hours Ago   \n",
      "5   How a 28-year-old in Norway prepares for a req...   5 Hours Ago   \n",
      "6   10 rules of ikigai, from authors of the Japane...   5 Hours Ago   \n",
      "7   5 low-stress summer jobs that are hiring right...   5 Hours Ago   \n",
      "8   Americans need to earn over $230,000 a year to...   6 Hours Ago   \n",
      "9   Bill Gates says Warren Buffett taught him how ...   6 Hours Ago   \n",
      "10  Now Boarding: Why is boarding an airplane so d...   7 Hours Ago   \n",
      "11  These stocks that are about to turn a profit s...   8 Hours Ago   \n",
      "12  Earnings playbook: JPMorgan Chase and Delta Ai...   8 Hours Ago   \n",
      "13  Yellen says Beijing talks a step forward in pu...  14 Hours Ago   \n",
      "14  U.S. soccer star Megan Rapinoe announces retir...  July 8, 2023   \n",
      "15  How to start a pet-sitting side hustle—you cou...  July 8, 2023   \n",
      "16  3 things to pay attention to in the markets th...  July 8, 2023   \n",
      "17  These U.S. states have the best and the worst ...  July 8, 2023   \n",
      "18  The No. 1 vitamin kids in the U.S. 'aren't get...  July 8, 2023   \n",
      "19  How the generative A.I. boom could forever cha...  July 8, 2023   \n",
      "20  The 10 U.S. cities where rent has increased th...  July 8, 2023   \n",
      "21  As 'bougie broke' videos trend, here's why exp...  July 8, 2023   \n",
      "22  The Fed rolls out a new economic index that co...  July 8, 2023   \n",
      "23  Active funds are powering JPMorgan's ETFs past...  July 8, 2023   \n",
      "24  Micron and Pfizer are the most oversold S&P 50...  July 8, 2023   \n",
      "25  Medicare will pay for Alzheimer's drug Leqembi...  July 8, 2023   \n",
      "26  Buy Buy Baby auction is canceled, but buyers a...  July 7, 2023   \n",
      "27  Elon Musk's Twitter sues top law firm Wachtell...  July 7, 2023   \n",
      "28  Biogen falls after Alzheimer’s drug approval. ...  July 7, 2023   \n",
      "29  Justin Bieber's Bored Ape NFT has lost about 9...  July 7, 2023   \n",
      "\n",
      "                                             Newslink  \n",
      "0   https://www.cnbc.com/2023/07/09/chris-christie...  \n",
      "1   https://www.cnbc.com/2023/07/09/marriage-thera...  \n",
      "2   https://www.cnbc.com/2023/07/09/republican-can...  \n",
      "3   https://www.cnbc.com/2023/07/09/yellen-had-a-c...  \n",
      "4   https://www.cnbc.com/2023/07/09/cramer-advises...  \n",
      "5   https://www.cnbc.com/2023/07/09/how-a-norway-2...  \n",
      "6   https://www.cnbc.com/2023/07/09/10-rules-of-ik...  \n",
      "7   https://www.cnbc.com/2023/07/09/5-low-stress-s...  \n",
      "8   https://www.cnbc.com/2023/07/09/salary-america...  \n",
      "9   https://www.cnbc.com/2023/07/09/bill-gates-say...  \n",
      "10  https://www.cnbc.com/2023/07/09/now-boarding-w...  \n",
      "11  https://www.cnbc.com/2023/07/09/these-stocks-t...  \n",
      "12  https://www.cnbc.com/2023/07/09/earnings-playb...  \n",
      "13  https://www.cnbc.com/2023/07/09/janet-yellen-c...  \n",
      "14  https://www.cnbc.com/2023/07/08/us-soccer-star...  \n",
      "15  https://www.cnbc.com/2023/07/08/how-to-start-a...  \n",
      "16  https://www.cnbc.com/2023/07/08/3-things-to-pa...  \n",
      "17  https://www.cnbc.com/2023/07/08/best-and-worst...  \n",
      "18  https://www.cnbc.com/2023/07/08/no-1-nutrient-...  \n",
      "19  https://www.cnbc.com/2023/07/08/how-the-genera...  \n",
      "20  https://www.cnbc.com/2023/07/08/us-cities-wher...  \n",
      "21  https://www.cnbc.com/2023/07/08/as-bougie-brok...  \n",
      "22  https://www.cnbc.com/2023/07/08/the-fed-has-ro...  \n",
      "23  https://www.cnbc.com/2023/07/08/active-funds-a...  \n",
      "24  https://www.cnbc.com/2023/07/08/micron-and-pfi...  \n",
      "25  https://www.cnbc.com/2023/07/08/leqembi-and-me...  \n",
      "26  https://www.cnbc.com/2023/07/07/buy-buy-baby-a...  \n",
      "27  https://www.cnbc.com/2023/07/07/elon-musks-twi...  \n",
      "28  https://www.cnbc.com/2023/07/07/biogen-falls-a...  \n",
      "29  https://www.cnbc.com/2023/07/07/justin-biebers...  \n"
     ]
    }
   ],
   "source": [
    "def Webpage(url):\n",
    "    link = requests.get(url)\n",
    "    \n",
    "    Soup = BeautifulSoup(link.content)\n",
    "    \n",
    "    Headlines = []\n",
    "    for i in Soup.find_all('a', class_=\"LatestNews-headline\"):\n",
    "        Headlines.append(i.text)\n",
    "    \n",
    "    Time = []\n",
    "    for i in Soup.find_all('time', class_=\"LatestNews-timestamp\"):\n",
    "        Time.append(i.text)\n",
    "    \n",
    "    Newslink = []\n",
    "    for i in Soup.find_all('a', class_=\"LatestNews-headline\"):\n",
    "        Newslink.append(i.get('href'))\n",
    "            \n",
    "    df = pd.DataFrame({'Headlines':Headlines, 'Time':Time, 'Newslink':Newslink})\n",
    "    print(df)\n",
    "    \n",
    "Webpage('https://www.cnbc.com/world/?region=world')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf84d279",
   "metadata": {},
   "source": [
    "# 6) Write a python program to scrape the details of most downloaded articles from AI in last 90days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articlesScrape below mentioned details and make data frame.i) Paper Title ii) Authors iii) Published Date iv) Paper URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0425aa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Paper Title  \\\n",
      "0                                    Reward is enough   \n",
      "1   Explanation in artificial intelligence: Insigh...   \n",
      "2              Creativity and artificial intelligence   \n",
      "3   Conflict-based search for optimal multi-agent ...   \n",
      "4   Knowledge graphs as tools for explainable mach...   \n",
      "5   Law and logic: A review from an argumentation ...   \n",
      "6   Between MDPs and semi-MDPs: A framework for te...   \n",
      "7   Explaining individual predictions when feature...   \n",
      "8       Multiple object tracking: A literature review   \n",
      "9   A survey of inverse reinforcement learning: Ch...   \n",
      "10  Evaluating XAI: A comparison of rule-based and...   \n",
      "11  Explainable AI tools for legal reasoning about...   \n",
      "12            Hard choices in artificial intelligence   \n",
      "13  Assessing the communication gap between AI mod...   \n",
      "14  Explaining black-box classifiers using post-ho...   \n",
      "15  The Hanabi challenge: A new frontier for AI re...   \n",
      "16              Wrappers for feature subset selection   \n",
      "17  Artificial cognition for social human–robot in...   \n",
      "18  A review of possible effects of cognitive bias...   \n",
      "19  The multifaceted impact of Ada Lovelace in the...   \n",
      "20  Robot ethics: Mapping the issues for a mechani...   \n",
      "21          Reward (Mis)design for autonomous driving   \n",
      "22  Planning and acting in partially observable st...   \n",
      "23  What do we want from Explainable Artificial In...   \n",
      "\n",
      "                                              Authors  Published Date  \\\n",
      "0   David Silver, Satinder Singh, Doina Precup, Ri...    October 2021   \n",
      "1                                         Tim Miller    February 2019   \n",
      "2                                  Margaret A. Boden      August 1998   \n",
      "3   Guni Sharon, Roni Stern, Ariel Felner, Nathan ...   February 2015   \n",
      "4                     Ilaria Tiddi, Stefan Schlobach     January 2022   \n",
      "5                     Henry Prakken, Giovanni Sartor     October 2015   \n",
      "6    Richard S. Sutton, Doina Precup, Satinder Singh      August 1999   \n",
      "7          Kjersti Aas, Martin Jullum, Anders Løland   September 2021   \n",
      "8                Wenhan Luo, Junliang Xing and 4 more      April 2021   \n",
      "9                      Saurabh Arora, Prashant Doshi      August 2021   \n",
      "10  Jasper van der Waa, Elisabeth Nieuwburg, Anita...   February 2021   \n",
      "11  Joe Collenette, Katie Atkinson, Trevor Bench-C...      April 2023   \n",
      "12  Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz    November 2021   \n",
      "13  Oskar Wysocki, Jessica Katharine Davies and 5 ...      March 2023   \n",
      "14  Eoin M. Kenny, Courtney Ford, Molly Quinn, Mar...        May 2021   \n",
      "15          Nolan Bard, Jakob N. Foerster and 13 more      March 2020   \n",
      "16                        Ron Kohavi, George H. John    December 1997   \n",
      "17      Séverin Lemaignan, Mathieu Warnier and 3 more       June 2017   \n",
      "18   Tomáš Kliegr, Štěpán Bahník, Johannes Fürnkranz        June 2021   \n",
      "19                            Luigia Carlucci Aiello        June 2016   \n",
      "20            Patrick Lin, Keith Abney, George Bekey       April 2011   \n",
      "21     W. Bradley Knox, Alessandro Allievi and 3 more      March 2023   \n",
      "22  Leslie Pack Kaelbling, Michael L. Littman, Ant...        May 1998   \n",
      "23             Markus Langer, Daniel Oster and 6 more       July 2021   \n",
      "\n",
      "                                            Paper URL  \n",
      "0   https://www.sciencedirect.com/science/article/...  \n",
      "1   https://www.sciencedirect.com/science/article/...  \n",
      "2   https://www.sciencedirect.com/science/article/...  \n",
      "3   https://www.sciencedirect.com/science/article/...  \n",
      "4   https://www.sciencedirect.com/science/article/...  \n",
      "5   https://www.sciencedirect.com/science/article/...  \n",
      "6   https://www.sciencedirect.com/science/article/...  \n",
      "7   https://www.sciencedirect.com/science/article/...  \n",
      "8   https://www.sciencedirect.com/science/article/...  \n",
      "9   https://www.sciencedirect.com/science/article/...  \n",
      "10  https://www.sciencedirect.com/science/article/...  \n",
      "11  https://www.sciencedirect.com/science/article/...  \n",
      "12  https://www.sciencedirect.com/science/article/...  \n",
      "13  https://www.sciencedirect.com/science/article/...  \n",
      "14  https://www.sciencedirect.com/science/article/...  \n",
      "15  https://www.sciencedirect.com/science/article/...  \n",
      "16  https://www.sciencedirect.com/science/article/...  \n",
      "17  https://www.sciencedirect.com/science/article/...  \n",
      "18  https://www.sciencedirect.com/science/article/...  \n",
      "19  https://www.sciencedirect.com/science/article/...  \n",
      "20  https://www.sciencedirect.com/science/article/...  \n",
      "21  https://www.sciencedirect.com/science/article/...  \n",
      "22  https://www.sciencedirect.com/science/article/...  \n",
      "23  https://www.sciencedirect.com/science/article/...  \n"
     ]
    }
   ],
   "source": [
    "def Webpage(url):\n",
    "    link = requests.get(url)\n",
    "    \n",
    "    Soup = BeautifulSoup(link.content)\n",
    "    \n",
    "    PaperTitle = []\n",
    "    for i in Soup.find_all('h2', class_=\"sc-1qrq3sd-1 gRGSUS sc-1nmom32-0 sc-1nmom32-1 btcbYu goSKRg\"):\n",
    "        PaperTitle.append(i.text)\n",
    "    \n",
    "    Authors = []\n",
    "    for i in Soup.find_all('span', class_=\"sc-1w3fpd7-0 dnCnAO\"):\n",
    "        Authors.append(i.text)\n",
    "        \n",
    "    PublishedDate = []\n",
    "    for i in Soup.find_all('span', class_=\"sc-1thf9ly-2 dvggWt\"):\n",
    "        PublishedDate.append(i.text)\n",
    "    \n",
    "    PaperURL = []\n",
    "    for i in Soup.find_all('a', class_=\"sc-5smygv-0 fIXTHm\"):\n",
    "        PaperURL.append(i.get('href'))\n",
    "            \n",
    "    df = pd.DataFrame({'Paper Title':PaperTitle, 'Authors':Authors, 'Published Date':PublishedDate, 'Paper URL':PaperURL})\n",
    "    print(df)\n",
    "    \n",
    "Webpage('https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a98be48",
   "metadata": {},
   "source": [
    "# 7) Write a python program to scrape mentioned details from dineout.co.inand make data frame\u0002i) Restaurant nameii) Cuisineiii) Locationiv) Ratingsv) Image URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb030ce",
   "metadata": {},
   "source": [
    "This Website is not accessable from UK. Showing this message :- (Food not found!\n",
    "Looks like you are not in our service area but don't worry, we take uniting foodies with food very seriously and will come to your region soon!\n",
    "\n",
    "Are you in our service area?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
